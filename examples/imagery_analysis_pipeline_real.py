"""
å«æ˜Ÿå½±åƒåˆ†æžæµæ°´çº¿ - çœŸå®žLLMç‰ˆæœ¬
================================

ä½¿ç”¨ StateGraph å®žçŽ°å¤šæ¨¡åž‹åä½œçš„å½±åƒåˆ†æžæµæ°´çº¿ï¼Œè°ƒç”¨çœŸå®žçš„å¤§æ¨¡åž‹APIï¼š
1. å›¾åƒåŠ è½½ -> 2. åˆ†å‰²æ¨¡åž‹(LLMæ¨¡æ‹Ÿ) -> 3. å¤§æ¨¡åž‹åˆ†æž -> 4. å†™ä½œæ¨¡åž‹ -> 5. ç”ŸæˆæŠ¥å‘Š

è¿è¡Œæ–¹å¼:
    python examples/imagery_analysis_pipeline_real.py
"""
import asyncio
import os
import base64
import json
from datetime import datetime
from pathlib import Path
from typing import TypedDict, List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from swagent.stategraph import StateGraph, ExecutionConfig, GraphStatus
from swagent.llm import OpenAIClient, LLMConfig


# =============================================================================
# çŠ¶æ€å®šä¹‰
# =============================================================================

class ImageAnalysisState(TypedDict):
    """å½±åƒåˆ†æžæµæ°´çº¿çŠ¶æ€"""
    # è¾“å…¥
    image_path: str
    image_name: str

    # å›¾åƒæ•°æ®
    image_base64: Optional[str]
    image_size_mb: float

    # åˆ†å‰²æ¨¡åž‹è¾“å‡º
    segmentation_result: Optional[str]
    land_cover_stats: Optional[Dict[str, float]]

    # å¤§æ¨¡åž‹åˆ†æžè¾“å‡º
    scene_analysis: Optional[str]
    detected_features: Optional[str]

    # å†™ä½œæ¨¡åž‹è¾“å‡º
    report_draft: Optional[str]

    # æœ€ç»ˆè¾“å‡º
    final_report: Optional[str]
    report_path: Optional[str]

    # å…ƒæ•°æ®
    processing_log: List[str]
    start_time: str
    llm_calls: int
    total_tokens: int
    errors: List[str]


# =============================================================================
# LLM å®¢æˆ·ç«¯ç®¡ç†
# =============================================================================

class LLMManager:
    """LLMå®¢æˆ·ç«¯ç®¡ç†å™¨"""

    def __init__(self):
        self.client: Optional[OpenAIClient] = None
        self.total_tokens = 0

    async def init_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            self.client = OpenAIClient.from_config_file()
            return True
        except Exception as e:
            print(f"âŒ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def chat(
        self,
        messages: List[Dict[str, Any]],
        temperature: float = 0.7,
        max_tokens: int = 2048
    ) -> Optional[str]:
        """è°ƒç”¨LLM"""
        if not self.client:
            return None

        try:
            response = await self.client.chat(
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            self.total_tokens += response.usage.get("total_tokens", 0)
            return response.content
        except Exception as e:
            print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            return None


# å…¨å±€LLMç®¡ç†å™¨
llm_manager = LLMManager()


# =============================================================================
# StateGraph æµæ°´çº¿å®šä¹‰
# =============================================================================

def create_real_imagery_pipeline():
    """åˆ›å»ºçœŸå®žLLMè°ƒç”¨çš„å½±åƒåˆ†æžæµæ°´çº¿"""

    graph = StateGraph(ImageAnalysisState)

    # ----- èŠ‚ç‚¹1: åŠ è½½å›¾åƒ -----
    @graph.node()
    async def load_image(state: ImageAnalysisState) -> dict:
        """åŠ è½½å›¾åƒå¹¶è½¬æ¢ä¸ºbase64"""
        image_path = state["image_path"]

        if not os.path.exists(image_path):
            return {
                "errors": state["errors"] + [f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}"],
                "processing_log": state["processing_log"] + ["[ERROR] å›¾åƒåŠ è½½å¤±è´¥"]
            }

        # è¯»å–å›¾åƒå¹¶è½¬æ¢ä¸ºbase64
        with open(image_path, "rb") as f:
            image_data = f.read()

        image_base64 = base64.b64encode(image_data).decode("utf-8")
        file_size_mb = len(image_data) / 1024 / 1024

        return {
            "image_base64": image_base64,
            "image_size_mb": file_size_mb,
            "image_name": os.path.basename(image_path),
            "processing_log": state["processing_log"] + [
                f"[OK] å›¾åƒåŠ è½½å®Œæˆ: {os.path.basename(image_path)}",
                f"     æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB"
            ]
        }

    # ----- èŠ‚ç‚¹2: åˆ†å‰²æ¨¡åž‹ (LLMæ¨¡æ‹Ÿ) -----
    @graph.node()
    async def run_segmentation(state: ImageAnalysisState) -> dict:
        """ä½¿ç”¨LLMæ¨¡æ‹Ÿåˆ†å‰²æ¨¡åž‹ï¼Œåˆ†æžåœ°ç‰©ç±»åž‹"""
        if state["image_base64"] is None:
            return {
                "errors": state["errors"] + ["æ— å›¾åƒæ•°æ®"],
                "processing_log": state["processing_log"] + ["[SKIP] åˆ†å‰²åˆ†æž"]
            }

        print("  ðŸ”„ æ­£åœ¨è¿›è¡Œåœ°ç‰©åˆ†å‰²åˆ†æž...")

        # æž„å»ºæ¶ˆæ¯ - è®©LLMåˆ†æžå›¾åƒä¸­çš„åœ°ç‰©ç±»åž‹
        messages = [
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é¥æ„Ÿå½±åƒåˆ†æžä¸“å®¶ï¼Œæ“…é•¿åˆ†æžå«æ˜Ÿå½±åƒä¸­çš„åœ°ç‰©ç±»åž‹ã€‚
è¯·åˆ†æžç”¨æˆ·æä¾›çš„å«æ˜Ÿå½±åƒï¼Œè¯†åˆ«å¹¶ä¼°ç®—å„ç±»åœ°ç‰©çš„å æ¯”ã€‚

ä½ éœ€è¦è¾“å‡ºä»¥ä¸‹æ ¼å¼çš„JSONç»“æžœï¼š
{
    "land_cover": {
        "building": 0.XX,
        "road": 0.XX,
        "vegetation": 0.XX,
        "water": 0.XX,
        "bare_land": 0.XX,
        "other": 0.XX
    },
    "analysis": "è¯¦ç»†çš„åœ°ç‰©åˆ†æžæè¿°..."
}

æ³¨æ„ï¼šæ‰€æœ‰å æ¯”ä¹‹å’Œåº”è¯¥ç­‰äºŽ1.0"""
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": f"è¯·åˆ†æžè¿™å¼ å«æ˜Ÿå½±åƒï¼ˆ{state['image_name']}ï¼‰ï¼Œè¯†åˆ«å…¶ä¸­çš„åœ°ç‰©ç±»åž‹å¹¶ä¼°ç®—å„ç±»åœ°ç‰©çš„è¦†ç›–æ¯”ä¾‹ã€‚"
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{state['image_base64']}"
                        }
                    }
                ]
            }
        ]

        response = await llm_manager.chat(messages, temperature=0.3, max_tokens=1024)

        if not response:
            return {
                "errors": state["errors"] + ["åˆ†å‰²æ¨¡åž‹è°ƒç”¨å¤±è´¥"],
                "processing_log": state["processing_log"] + ["[ERROR] åˆ†å‰²åˆ†æžå¤±è´¥"],
                "llm_calls": state["llm_calls"] + 1
            }

        # å°è¯•è§£æžJSON
        land_cover = {}
        try:
            # å°è¯•ä»Žå“åº”ä¸­æå–JSON
            import re
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                result = json.loads(json_match.group())
                land_cover = result.get("land_cover", {})
        except:
            pass

        return {
            "segmentation_result": response,
            "land_cover_stats": land_cover if land_cover else {
                "building": 0.40, "road": 0.20, "vegetation": 0.25,
                "bare_land": 0.10, "other": 0.05
            },
            "processing_log": state["processing_log"] + [
                "[OK] åœ°ç‰©åˆ†å‰²åˆ†æžå®Œæˆ",
                f"     è¯†åˆ«åœ°ç‰©ç±»åž‹: {len(land_cover) if land_cover else 5} ç±»"
            ],
            "llm_calls": state["llm_calls"] + 1,
            "total_tokens": llm_manager.total_tokens
        }

    # ----- èŠ‚ç‚¹3: å¤§æ¨¡åž‹åœºæ™¯åˆ†æž -----
    @graph.node()
    async def run_scene_analysis(state: ImageAnalysisState) -> dict:
        """ä½¿ç”¨å¤§æ¨¡åž‹è¿›è¡Œåœºæ™¯åˆ†æž"""
        if state["image_base64"] is None:
            return {
                "errors": state["errors"] + ["æ— å›¾åƒæ•°æ®"],
                "processing_log": state["processing_log"] + ["[SKIP] åœºæ™¯åˆ†æž"]
            }

        print("  ðŸ”„ æ­£åœ¨è¿›è¡Œåœºæ™¯åˆ†æž...")

        # æž„å»ºåœºæ™¯åˆ†æžprompt
        land_cover_info = ""
        if state["land_cover_stats"]:
            land_cover_info = "åœ°ç‰©åˆ†å‰²ç»“æžœï¼š\n"
            for k, v in state["land_cover_stats"].items():
                land_cover_info += f"  - {k}: {v*100:.1f}%\n"

        messages = [
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„åŸŽå¸‚è§„åˆ’å’Œé¥æ„Ÿåˆ†æžä¸“å®¶ã€‚è¯·å¯¹å«æ˜Ÿå½±åƒè¿›è¡Œæ·±å…¥çš„åœºæ™¯åˆ†æžï¼ŒåŒ…æ‹¬ï¼š

1. **åœºæ™¯ç±»åž‹åˆ¤æ–­**ï¼šåˆ¤æ–­è¿™æ˜¯ä»€ä¹ˆç±»åž‹çš„åŒºåŸŸï¼ˆå±…ä½åŒºã€å•†ä¸šåŒºã€å·¥ä¸šåŒºã€å†œä¸šåŒºç­‰ï¼‰
2. **å»ºç­‘ç‰¹å¾åˆ†æž**ï¼šåˆ†æžå»ºç­‘ç‰©çš„ç±»åž‹ã€å¯†åº¦ã€å¸ƒå±€ç‰¹ç‚¹
3. **äº¤é€šè®¾æ–½åˆ†æž**ï¼šåˆ†æžé“è·¯ç½‘ç»œã€åœè½¦è®¾æ–½ç­‰
4. **ç»¿åŒ–çŽ¯å¢ƒåˆ†æž**ï¼šåˆ†æžæ¤è¢«è¦†ç›–ã€ç»¿åœ°åˆ†å¸ƒ
5. **ç‰¹æ®Šè®¾æ–½è¯†åˆ«**ï¼šè¯†åˆ«å­¦æ ¡ã€åŒ»é™¢ã€å…¬å›­ã€ä½“è‚²åœºç­‰ç‰¹æ®Šè®¾æ–½
6. **åŒºåŸŸç‰¹ç‚¹æ€»ç»“**ï¼šæ€»ç»“è¯¥åŒºåŸŸçš„ä¸»è¦ç‰¹ç‚¹å’Œå‘å±•çŠ¶å†µ

è¯·æä¾›è¯¦ç»†ã€ä¸“ä¸šçš„åˆ†æžæŠ¥å‘Šã€‚"""
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": f"""è¯·å¯¹è¿™å¼ å«æ˜Ÿå½±åƒè¿›è¡Œè¯¦ç»†çš„åœºæ™¯åˆ†æžã€‚

å½±åƒä¿¡æ¯ï¼š
- æ–‡ä»¶åï¼š{state['image_name']}
- æ–‡ä»¶å¤§å°ï¼š{state['image_size_mb']:.2f} MB

{land_cover_info}

è¯·ä»ŽåŸŽå¸‚è§„åˆ’å’Œé¥æ„Ÿåˆ†æžçš„ä¸“ä¸šè§’åº¦ï¼Œå¯¹è¯¥åŒºåŸŸè¿›è¡Œå…¨é¢åˆ†æžã€‚"""
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{state['image_base64']}"
                        }
                    }
                ]
            }
        ]

        response = await llm_manager.chat(messages, temperature=0.7, max_tokens=2048)

        if not response:
            return {
                "errors": state["errors"] + ["åœºæ™¯åˆ†æžè°ƒç”¨å¤±è´¥"],
                "processing_log": state["processing_log"] + ["[ERROR] åœºæ™¯åˆ†æžå¤±è´¥"],
                "llm_calls": state["llm_calls"] + 1
            }

        return {
            "scene_analysis": response,
            "processing_log": state["processing_log"] + [
                "[OK] åœºæ™¯åˆ†æžå®Œæˆ",
                f"     åˆ†æžé•¿åº¦: {len(response)} å­—ç¬¦"
            ],
            "llm_calls": state["llm_calls"] + 1,
            "total_tokens": llm_manager.total_tokens
        }

    # ----- èŠ‚ç‚¹4: ç‰¹å¾æ£€æµ‹åˆ†æž -----
    @graph.node()
    async def run_feature_detection(state: ImageAnalysisState) -> dict:
        """æ£€æµ‹å’Œåˆ†æžå…·ä½“ç‰¹å¾"""
        if state["image_base64"] is None:
            return {
                "errors": state["errors"] + ["æ— å›¾åƒæ•°æ®"],
                "processing_log": state["processing_log"] + ["[SKIP] ç‰¹å¾æ£€æµ‹"]
            }

        print("  ðŸ”„ æ­£åœ¨è¿›è¡Œç‰¹å¾æ£€æµ‹...")

        messages = [
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é¥æ„Ÿå›¾åƒç›®æ ‡æ£€æµ‹ä¸“å®¶ã€‚è¯·å¯¹å«æ˜Ÿå½±åƒè¿›è¡Œç›®æ ‡æ£€æµ‹å’Œç‰¹å¾åˆ†æžï¼š

1. **å»ºç­‘ç‰©æ£€æµ‹**ï¼š
   - ä¼°ç®—å»ºç­‘ç‰©æ•°é‡
   - åŒºåˆ†å»ºç­‘ç±»åž‹ï¼ˆä½å®…ã€å•†ä¸šã€å·¥ä¸šã€å…¬å…±è®¾æ–½ç­‰ï¼‰
   - åˆ†æžå»ºç­‘é«˜åº¦ç‰¹å¾

2. **é“è·¯æ£€æµ‹**ï¼š
   - è¯†åˆ«ä¸»å¹²é“å’Œæ”¯è·¯
   - åˆ†æžé“è·¯ç½‘ç»œç»“æž„
   - ä¼°ç®—é“è·¯å¯†åº¦

3. **ç»¿åœ°æ£€æµ‹**ï¼š
   - è¯†åˆ«å…¬å›­ã€ç»¿åœ°
   - åˆ†æžæ¤è¢«åˆ†å¸ƒç‰¹ç‚¹

4. **ç‰¹æ®Šè®¾æ–½**ï¼š
   - è¯†åˆ«ä½“è‚²åœºã€å­¦æ ¡ã€åœè½¦åœºç­‰
   - æ ‡æ³¨ç‰¹æ®Šå»ºç­‘ç‰©

è¯·ä»¥ç»“æž„åŒ–çš„æ–¹å¼è¾“å‡ºæ£€æµ‹ç»“æžœã€‚"""
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": f"è¯·å¯¹è¿™å¼ å«æ˜Ÿå½±åƒï¼ˆ{state['image_name']}ï¼‰è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œè¯†åˆ«å„ç±»åœ°ç‰©ç›®æ ‡å¹¶ç»™å‡ºæ•°é‡ä¼°è®¡ã€‚"
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{state['image_base64']}"
                        }
                    }
                ]
            }
        ]

        response = await llm_manager.chat(messages, temperature=0.5, max_tokens=1536)

        if not response:
            return {
                "errors": state["errors"] + ["ç‰¹å¾æ£€æµ‹è°ƒç”¨å¤±è´¥"],
                "processing_log": state["processing_log"] + ["[ERROR] ç‰¹å¾æ£€æµ‹å¤±è´¥"],
                "llm_calls": state["llm_calls"] + 1
            }

        return {
            "detected_features": response,
            "processing_log": state["processing_log"] + [
                "[OK] ç‰¹å¾æ£€æµ‹å®Œæˆ",
                f"     æ£€æµ‹ç»“æžœé•¿åº¦: {len(response)} å­—ç¬¦"
            ],
            "llm_calls": state["llm_calls"] + 1,
            "total_tokens": llm_manager.total_tokens
        }

    # ----- èŠ‚ç‚¹5: å†™ä½œæ¨¡åž‹ç”ŸæˆæŠ¥å‘Š -----
    @graph.node()
    async def run_report_writing(state: ImageAnalysisState) -> dict:
        """ä½¿ç”¨å†™ä½œæ¨¡åž‹ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""
        print("  ðŸ”„ æ­£åœ¨ç”Ÿæˆåˆ†æžæŠ¥å‘Š...")

        # æ±‡æ€»ä¹‹å‰çš„åˆ†æžç»“æžœ
        previous_analysis = f"""
## åœ°ç‰©åˆ†å‰²ç»“æžœ

{state.get('segmentation_result', 'æ— åˆ†å‰²ç»“æžœ')}

## åœºæ™¯åˆ†æžç»“æžœ

{state.get('scene_analysis', 'æ— åœºæ™¯åˆ†æž')}

## ç‰¹å¾æ£€æµ‹ç»“æžœ

{state.get('detected_features', 'æ— ç‰¹å¾æ£€æµ‹ç»“æžœ')}
"""

        messages = [
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æŠ€æœ¯æŠ¥å‘Šæ’°å†™ä¸“å®¶ï¼Œæ“…é•¿å°†é¥æ„Ÿåˆ†æžç»“æžœæ•´ç†æˆç»“æž„åŒ–çš„ä¸“ä¸šæŠ¥å‘Šã€‚

è¯·æ ¹æ®æä¾›çš„åˆ†æžç»“æžœï¼Œç”Ÿæˆä¸€ä»½å®Œæ•´çš„å«æ˜Ÿå½±åƒåˆ†æžæŠ¥å‘Šï¼ŒåŒ…å«ä»¥ä¸‹ç« èŠ‚ï¼š

1. **æ‰§è¡Œæ‘˜è¦** - ç®€è¦æ€»ç»“ä¸»è¦å‘çŽ°
2. **åˆ†æžæ–¹æ³•** - è¯´æ˜Žä½¿ç”¨çš„åˆ†æžæ–¹æ³•å’ŒæŠ€æœ¯
3. **åœ°ç‰©è¦†ç›–åˆ†æž** - è¯¦ç»†çš„åœŸåœ°åˆ©ç”¨/åœ°ç‰©è¦†ç›–åˆ†æž
4. **åœºæ™¯ç‰¹å¾åˆ†æž** - åŒºåŸŸç‰¹å¾å’Œç©ºé—´å¸ƒå±€åˆ†æž
5. **ç›®æ ‡æ£€æµ‹ç»“æžœ** - å„ç±»ç›®æ ‡çš„æ£€æµ‹ç»Ÿè®¡
6. **é—®é¢˜ä¸Žå»ºè®®** - å‘çŽ°çš„é—®é¢˜å’Œæ”¹è¿›å»ºè®®
7. **ç»“è®º** - æ€»ä½“ç»“è®º

æŠ¥å‘Šè¦æ±‚ï¼š
- ä½¿ç”¨Markdownæ ¼å¼
- è¯­è¨€ä¸“ä¸šã€å®¢è§‚
- æ•°æ®å‡†ç¡®ã€æœ‰ä¾æ®
- å»ºè®®å…·ä½“ã€å¯è¡Œ"""
            },
            {
                "role": "user",
                "content": f"""è¯·æ ¹æ®ä»¥ä¸‹åˆ†æžç»“æžœï¼Œç”Ÿæˆä¸€ä»½å®Œæ•´çš„å«æ˜Ÿå½±åƒåˆ†æžæŠ¥å‘Šã€‚

å½±åƒä¿¡æ¯ï¼š
- æ–‡ä»¶åï¼š{state['image_name']}
- åˆ†æžæ—¶é—´ï¼š{state['start_time']}

{previous_analysis}

è¯·ç”Ÿæˆä¸“ä¸šçš„åˆ†æžæŠ¥å‘Šã€‚"""
            }
        ]

        response = await llm_manager.chat(messages, temperature=0.7, max_tokens=3000)

        if not response:
            return {
                "errors": state["errors"] + ["æŠ¥å‘Šç”Ÿæˆè°ƒç”¨å¤±è´¥"],
                "processing_log": state["processing_log"] + ["[ERROR] æŠ¥å‘Šç”Ÿæˆå¤±è´¥"],
                "llm_calls": state["llm_calls"] + 1
            }

        return {
            "report_draft": response,
            "processing_log": state["processing_log"] + [
                "[OK] æŠ¥å‘Šåˆç¨¿ç”Ÿæˆå®Œæˆ",
                f"     æŠ¥å‘Šé•¿åº¦: {len(response)} å­—ç¬¦"
            ],
            "llm_calls": state["llm_calls"] + 1,
            "total_tokens": llm_manager.total_tokens
        }

    # ----- èŠ‚ç‚¹6: æ±‡æ€»ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š -----
    @graph.node()
    async def generate_final_report(state: ImageAnalysisState) -> dict:
        """æ±‡æ€»ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šå¹¶ä¿å­˜"""
        print("  ðŸ”„ æ­£åœ¨æ±‡æ€»æœ€ç»ˆæŠ¥å‘Š...")

        # æž„å»ºå®Œæ•´æŠ¥å‘Š
        report = f"""# å«æ˜Ÿå½±åƒæ™ºèƒ½åˆ†æžæŠ¥å‘Š

---

**å½±åƒæ–‡ä»¶**: {state['image_name']}
**åˆ†æžæ—¶é—´**: {state['start_time']}
**LLMè°ƒç”¨æ¬¡æ•°**: {state['llm_calls']}
**æ€»Tokenæ¶ˆè€—**: {state['total_tokens']}

---

{state.get('report_draft', 'æŠ¥å‘Šç”Ÿæˆå¤±è´¥')}

---

## é™„å½•ï¼šåŽŸå§‹åˆ†æžæ•°æ®

### A. åœ°ç‰©åˆ†å‰²åŽŸå§‹ç»“æžœ

{state.get('segmentation_result', 'N/A')[:1000]}...

### B. å¤„ç†æ—¥å¿—

```
{chr(10).join(state['processing_log'])}
```

---

*æœ¬æŠ¥å‘Šç”± StateGraph å¤šæ¨¡åž‹åä½œæµæ°´çº¿è‡ªåŠ¨ç”Ÿæˆ*
*ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""

        # ä¿å­˜æŠ¥å‘Š
        report_dir = Path(__file__).parent.parent / "reports"
        report_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        image_stem = Path(state['image_name']).stem
        report_filename = f"analysis_report_{image_stem}_{timestamp}.md"
        report_path = report_dir / report_filename

        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report)

        return {
            "final_report": report,
            "report_path": str(report_path),
            "processing_log": state["processing_log"] + [
                "[OK] æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ",
                f"     æŠ¥å‘Šè·¯å¾„: {report_path}"
            ],
            "total_tokens": llm_manager.total_tokens
        }

    # ----- å®šä¹‰æµæ°´çº¿æµç¨‹ -----
    graph.set_entry_point("load_image")
    graph.add_edge("load_image", "run_segmentation")
    graph.add_edge("run_segmentation", "run_scene_analysis")
    graph.add_edge("run_scene_analysis", "run_feature_detection")
    graph.add_edge("run_feature_detection", "run_report_writing")
    graph.add_edge("run_report_writing", "generate_final_report")
    graph.set_exit_point("generate_final_report")

    return graph


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("ðŸ›°ï¸  å«æ˜Ÿå½±åƒæ™ºèƒ½åˆ†æžæµæ°´çº¿ (çœŸå®žLLMç‰ˆæœ¬)")
    print("=" * 70)

    # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
    print("\nðŸ“¡ åˆå§‹åŒ–LLMå®¢æˆ·ç«¯...")
    if not await llm_manager.init_client():
        print("âŒ æ— æ³•åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return

    print(f"âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    print(f"   æ¨¡åž‹: {llm_manager.client.config.model}")
    print(f"   API: {llm_manager.client.config.base_url}")

    # æŸ¥æ‰¾å›¾åƒ
    imagery_dir = Path(__file__).parent.parent / "imagery_output"

    if not imagery_dir.exists():
        print(f"âŒ å›¾åƒç›®å½•ä¸å­˜åœ¨: {imagery_dir}")
        return

    # èŽ·å–å›¾åƒåˆ—è¡¨
    images = list(imagery_dir.glob("*.png")) + list(imagery_dir.glob("*.jpg"))

    if not images:
        print(f"âŒ æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
        return

    print(f"\nðŸ“‚ æ‰¾åˆ° {len(images)} ä¸ªå›¾åƒæ–‡ä»¶:")
    for i, img in enumerate(images):
        size_mb = img.stat().st_size / 1024 / 1024
        print(f"   {i+1}. {img.name} ({size_mb:.2f} MB)")

    # é€‰æ‹©è¾ƒå°çš„å›¾åƒï¼ˆé¿å…base64è¿‡å¤§ï¼‰
    images_sorted = sorted(images, key=lambda x: x.stat().st_size)
    selected_image = images_sorted[0]

    # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¤§äºŽ10MBå¯èƒ½ä¼šæœ‰é—®é¢˜
    if selected_image.stat().st_size > 10 * 1024 * 1024:
        print(f"\nâš ï¸  è­¦å‘Š: å›¾åƒæ–‡ä»¶è¾ƒå¤§ ({selected_image.stat().st_size/1024/1024:.1f} MB)ï¼Œå¯èƒ½å½±å“å¤„ç†é€Ÿåº¦")

    print(f"\nðŸŽ¯ é€‰æ‹©åˆ†æž: {selected_image.name}")

    # åˆ›å»ºæµæ°´çº¿
    print("\nðŸ“Š åˆ›å»ºåˆ†æžæµæ°´çº¿...")
    graph = create_real_imagery_pipeline()

    # éªŒè¯å›¾
    validation_errors = graph.validate()
    if validation_errors:
        print(f"âŒ æµæ°´çº¿éªŒè¯å¤±è´¥: {validation_errors}")
        return

    print("âœ… æµæ°´çº¿éªŒè¯é€šè¿‡")
    print(f"   èŠ‚ç‚¹æ•°: {len(graph._nodes)}")

    # ç¼–è¯‘
    config = ExecutionConfig(max_iterations=10, save_checkpoints=True)
    app = graph.compile(config=config)

    # åˆå§‹çŠ¶æ€
    initial_state: ImageAnalysisState = {
        "image_path": str(selected_image),
        "image_name": selected_image.name,
        "image_base64": None,
        "image_size_mb": 0.0,
        "segmentation_result": None,
        "land_cover_stats": None,
        "scene_analysis": None,
        "detected_features": None,
        "report_draft": None,
        "final_report": None,
        "report_path": None,
        "processing_log": [],
        "start_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "llm_calls": 0,
        "total_tokens": 0,
        "errors": []
    }

    # æ‰§è¡Œæµæ°´çº¿
    print("\nðŸš€ å¼€å§‹æ‰§è¡Œæµæ°´çº¿...\n")
    print("-" * 70)

    start_time = datetime.now()
    result = await app.invoke(initial_state)
    end_time = datetime.now()

    # è¾“å‡ºç»“æžœ
    print("-" * 70)
    print("\n" + "=" * 70)
    print("ðŸ“‹ æ‰§è¡Œç»“æžœ")
    print("=" * 70)

    print(f"\nçŠ¶æ€: {result.status.value}")
    print(f"è¿­ä»£æ¬¡æ•°: {result.iterations}")
    print(f"æ‰§è¡Œæ—¶é—´: {(end_time - start_time).total_seconds():.1f} ç§’")
    print(f"LLMè°ƒç”¨æ¬¡æ•°: {result.state['llm_calls']}")
    print(f"æ€»Tokenæ¶ˆè€—: {result.state['total_tokens']}")

    if result.error:
        print(f"\nâŒ é”™è¯¯: {result.error}")

    print("\nðŸ“ å¤„ç†æ—¥å¿—:")
    for log in result.state["processing_log"]:
        print(f"   {log}")

    if result.state["errors"]:
        print("\nâš ï¸ é”™è¯¯ä¿¡æ¯:")
        for err in result.state["errors"]:
            print(f"   - {err}")

    if result.state["report_path"]:
        print(f"\nðŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {result.state['report_path']}")

        # æ˜¾ç¤ºæŠ¥å‘Šé¢„è§ˆ
        print("\n" + "=" * 70)
        print("ðŸ“– æŠ¥å‘Šé¢„è§ˆ")
        print("=" * 70)
        report_preview = result.state["final_report"][:3000]
        print(report_preview)
        if len(result.state["final_report"]) > 3000:
            print("\n... (æŠ¥å‘Šå·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹è¯·æŸ¥çœ‹æ–‡ä»¶)")

    return result


if __name__ == "__main__":
    asyncio.run(main())
